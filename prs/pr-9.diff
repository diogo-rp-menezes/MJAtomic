diff --git a/src/agents/fullstack/agent.py b/src/agents/fullstack/agent.py
index a25762d..fa08066 100644
--- a/src/agents/fullstack/agent.py
+++ b/src/agents/fullstack/agent.py
@@ -1,71 +1,156 @@
-import logging
-from typing import Tuple, List
-from langgraph.prebuilt import create_react_agent
-from langchain_core.messages import HumanMessage
-from src.core.models import DevelopmentStep, TaskStatus
+from src.core.models import Step, TaskStatus
 from src.core.llm.provider import LLMProvider
-from src.tools.core_tools import core_tools
+from src.core.memory.vector_store import VectorMemory
+from src.core.memory.indexer import CodeIndexer
+from src.tools.file_io import FileIOTool
+from src.tools.secure_executor import SecureExecutorTool
+from src.core.logger import logger
+import os
+import yaml
+import json
+from typing import Tuple, List
 
 class FullstackAgent:
     def __init__(self,
                  workspace_path: str = "./workspace",
-                 llm_provider: LLMProvider = None):
+                 llm_provider: LLMProvider = None,
+                 file_io: FileIOTool = None,
+                 executor: SecureExecutorTool = None,
+                 memory = None,
+                 indexer = None):
 
         self.workspace_path = workspace_path
-        self.logger = logging.getLogger(self.__class__.__name__)
-        self.llm = llm_provider or LLMProvider(profile="smart")
 
-        # 1. Carregar o template do prompt
-        system_prompt = self._load_prompt_template("src/agents/fullstack/prompt.md")
+        # Inje√ß√£o de Depend√™ncia ou Default
+        self.llm = llm_provider or LLMProvider(profile="fast")
+        self.file_io = file_io or FileIOTool(root_path=self.workspace_path)
+        self.executor = executor or SecureExecutorTool(workspace_path=self.workspace_path)
+        self.memory = memory
+        self.indexer = indexer
+
+        # Carrega config se existir
+        self.config = self._load_config(os.path.join(workspace_path, "config.yaml"))
+
+        # Lazy Load de mem√≥ria se n√£o injetado e n√£o falhar
+        if self.memory is None:
+            try:
+                self.memory = VectorMemory()
+                self.indexer = CodeIndexer(workspace_path=self.workspace_path)
+            except:
+                logger.warning("Running FullstackAgent without Vector Memory (Degraded Mode)")
+
+    def _load_config(self, path: str) -> dict:
+        if os.path.exists(path):
+            try:
+                with open(path, 'r') as f: return yaml.safe_load(f)
+            except: pass
+        return {}
+
+    def _parse_and_save_files(self, data: dict) -> List[str]:
+        """Parses the LLM response data (dict) and saves files to disk. Returns list of saved filenames."""
+        created_files = []
+        files = data.get("files", [])
 
-        # 2. Criar o Agente ReAct com LangGraph
-        self.agent_executor = create_react_agent(self.llm.get_llm(), core_tools, state_modifier=system_prompt)
+        if not isinstance(files, list):
+            return []
 
-    def _load_prompt_template(self, file_path: str) -> str:
-        try:
-            with open(file_path, 'r', encoding='utf-8') as f:
-                return f.read()
-        except Exception as e:
-            self.logger.error(f"Erro ao carregar o prompt {file_path}: {e}")
-            raise
+        for f in files:
+            if isinstance(f, dict) and "filename" in f and "content" in f:
+                try:
+                    self.file_io.write_file(f["filename"], f["content"])
+                    created_files.append(f["filename"])
+                except Exception as e:
+                    logger.error(f"Failed to write file {f.get('filename')}: {e}")
 
-    def execute_step(self, step: DevelopmentStep, task_input: str) -> Tuple[DevelopmentStep, List[str]]:
-        self.logger.info(f"ü§ñ [Fullstack] Executando: {step.description}")
+        return created_files
+
+    def execute_step(self, step: Step, task_input: str = None) -> Tuple[Step, List[str]]:
+        logger.info(f"ü§ñ [Fullstack] Executing: {step.description}")
         step.status = TaskStatus.IN_PROGRESS
         modified_files = []
 
-        try:
-            # Invoca o grafo
-            # LangGraph espera um dict com "messages"
-            inputs = {"messages": [HumanMessage(content=task_input)]}
-
-            # invoke retorna o estado final
-            final_state = self.agent_executor.invoke(inputs)
-
-            messages = final_state.get("messages", [])
-
-            # Extra√ß√£o de arquivos modificados e resposta final
-            final_answer = ""
-            if messages:
-                final_answer = messages[-1].content
-
-                # Iterar para achar chamadas de ferramenta write_file
-                for msg in messages:
-                    if hasattr(msg, 'tool_calls'):
-                        for tool_call in msg.tool_calls:
-                            if tool_call['name'] == 'write_file':
-                                args = tool_call['args']
-                                if 'filename' in args:
-                                    modified_files.append(args['filename'])
-
-            step.status = TaskStatus.COMPLETED
-            step.result = "Tarefa conclu√≠da com sucesso."
-            step.logs = str(final_answer)
-            self.logger.info(f"‚úÖ [Fullstack] Conclu√≠do: {step.description}")
-
-        except Exception as e:
-            self.logger.error(f"‚ùå [Fullstack] Erro ao executar o passo: {e}")
-            step.status = TaskStatus.FAILED
-            step.result = f"Falha cr√≠tica durante a execu√ß√£o do agente: {str(e)}"
-
-        return step, list(set(modified_files))
+        # 1. Indexa√ß√£o r√°pida (Contexto)
+        if self.indexer:
+            try: self.indexer.index_workspace()
+            except: pass
+
+        # 2. Constru√ß√£o do Contexto
+        rag_context = ""
+        if self.memory:
+            try:
+                hits = self.memory.search(step.description, k=3)
+                for txt, meta in hits:
+                    rag_context += f"\nFile: {meta.get('source')}\n{txt}\n"
+            except: pass
+
+        # 3. Prompt System
+        system_prompt = """You are an Expert Fullstack Developer.
+        You must implement code or write tests based on the user request.
+
+        OUTPUT FORMAT (STRICT JSON):
+        {
+            "files": [
+                {"filename": "path/to/file.ext", "content": "code content..."}
+            ],
+            "command": "shell command to verify (e.g. pytest)"
+        }
+
+        Do not include markdown formatting (```json). Just raw JSON.
+        """
+
+        # Include task_input (feedback) if present
+        extra_input = f"\nADDITIONAL INPUT/FEEDBACK:\n{task_input}" if task_input else ""
+        current_context = f"TASK: {step.description}{extra_input}\n\nCONTEXT:\n{rag_context}"
+
+        attempts = 0
+        max_attempts = 3
+        history = ""
+
+        while attempts < max_attempts:
+            attempts += 1
+
+            # Chama LLM com modo JSON
+            response = self.llm.generate_response(
+                prompt=current_context + history,
+                system_message=system_prompt,
+                json_mode=True
+            )
+
+            try:
+                # Limpeza b√°sica caso o modelo teimoso mande markdown
+                clean_json = response.replace("```json", "").replace("```", "").strip()
+                data = json.loads(clean_json)
+
+                # 4. Side Effects (Escrever arquivos)
+                current_files = self._parse_and_save_files(data)
+
+                # 5. Verifica√ß√£o
+                cmd = data.get("command")
+                if not cmd:
+                    step.status = TaskStatus.COMPLETED
+                    step.result = "Done (No verification command)"
+                    step.logs = history
+                    return step, current_files
+
+                result = self.executor.run_command(cmd)
+
+                if result["exit_code"] == 0:
+                    step.status = TaskStatus.COMPLETED
+                    step.result = f"Success! Output: {result['output'][:100]}..."
+                    step.logs = history + "\n" + result["output"]
+                    return step, current_files
+                else:
+                    # Self-Healing: Adiciona erro ao contexto e tenta de novo
+                    history += f"\n\nATTEMPT {attempts} FAILED:\nCommand: {cmd}\nOutput: {result['output']}\n\nFix the code and try again."
+                    logger.info(f"Self-healing attempt {attempts}...")
+                    modified_files = current_files # Keep track, though we might overwrite next attempt
+
+            except json.JSONDecodeError:
+                history += "\n\nERROR: Invalid JSON response. Please format as valid JSON."
+            except Exception as e:
+                history += f"\n\nERROR: {str(e)}"
+
+        step.status = TaskStatus.FAILED
+        step.logs = history
+        step.result = "Failed after max attempts."
+        return step, modified_files
diff --git a/src/core/graph/workflow.py b/src/core/graph/workflow.py
index 2775e28..2fa8840 100644
--- a/src/core/graph/workflow.py
+++ b/src/core/graph/workflow.py
@@ -3,7 +3,7 @@
 from src.agents.tech_lead.agent import TechLeadAgent
 from src.agents.fullstack.agent import FullstackAgent
 from src.agents.reviewer.agent import CodeReviewAgent
-from src.core.models import TaskStatus, DevelopmentStep, DevelopmentPlan, AgentRole, Verdict
+from src.core.models import TaskStatus, DevelopmentStep, DevelopmentPlan, AgentRole, Verdict, CodeReviewVerdict
 from src.tools.core_tools import read_file
 from typing import Optional
 import uuid
@@ -55,7 +55,11 @@ def node_executor(state: AgentState) -> dict:
 
 
 def node_reviewer(state: AgentState) -> dict:
-    step = state["current_step"]
+    step = state.get("current_step")
+
+    if not step:
+        return {"review_verdict": CodeReviewVerdict(verdict=Verdict.FAIL, justification="Internal Error: Step not found in state")}
+
     modified_files = state.get("modified_files", [])
     
     if not modified_files:
diff --git a/tests/test_integration.py b/tests/test_integration.py
index ce7f970..7b7ab10 100644
--- a/tests/test_integration.py
+++ b/tests/test_integration.py
@@ -23,9 +23,10 @@ def setup_and_teardown(self):
     @patch("src.agents.tech_lead.agent.LLM")
     @patch("src.agents.tech_lead.agent.BaseAgent._load_prompt_template", return_value="Prompt")
     @patch("src.agents.reviewer.agent.LLMProvider")
-    @patch("src.agents.fullstack.agent.create_react_agent")
-    @patch("src.agents.fullstack.agent.AgentExecutor")
-    def test_full_chain_flow(self, mock_agent_executor, mock_create_react_agent, mock_reviewer_llm, mock_tech_base, mock_tech_llm, mock_fullstack_llm):
+    @patch("src.agents.fullstack.agent.SecureExecutorTool")
+    @patch("src.agents.fullstack.agent.VectorMemory") # Mock Memory
+    @patch("src.agents.fullstack.agent.CodeIndexer") # Mock Indexer
+    def test_full_chain_flow(self, mock_indexer, mock_memory, mock_secure_executor, mock_reviewer_llm, mock_tech_base, mock_tech_llm, mock_fullstack_llm):
         """
         Tests the complete chain:
         Tech Lead (Plan) -> Fullstack (Exec Step 1) -> Reviewer (Review Step 1)
@@ -37,26 +38,28 @@ def test_full_chain_flow(self, mock_agent_executor, mock_create_react_agent, moc
         mock_tech_llm_instance = mock_tech_llm.return_value
         from src.core.models import DevelopmentPlan, DevelopmentStep, AgentRole
 
-        mock_plan = DevelopmentPlan(
-            original_request="Create a TDD plan for math lib",
-            steps=[
-                DevelopmentStep(id="s1", description="Create test_math.py with failing test for add", role=AgentRole.FULLSTACK),
-                DevelopmentStep(id="s2", description="Implement add in math_lib.py", role=AgentRole.FULLSTACK)
+        # Tech Lead returns JSON plan
+        mock_plan_json = json.dumps({
+            "original_request": "Create a TDD plan for math lib",
+            "steps": [
+                {"id": "1", "description": "Create test_math.py", "role": "FULLSTACK"},
+                {"id": "2", "description": "Implement math_lib.py", "role": "FULLSTACK"}
             ]
-        )
-        mock_tech_llm_instance.generate_response.return_value = mock_plan.model_dump_json()
-
-        # Fullstack Mock (Agent Executor)
-        mock_executor_instance = mock_agent_executor.return_value
-        # Intermediate steps for Step 1 (simulate write_file)
-        mock_tool_call = MagicMock()
-        mock_tool_call.tool = "write_file"
-        mock_tool_call.tool_input = {"filename": "test_math.py"}
-
-        mock_executor_instance.invoke.return_value = {
-            "output": "Created test_math.py",
-            "intermediate_steps": [(mock_tool_call, "File written")]
-        }
+        })
+        mock_tech_llm_instance.generate_response.return_value = mock_plan_json
+
+        # Fullstack Mock
+        mock_fullstack_llm_instance = mock_fullstack_llm.return_value
+        # Fullstack returns JSON with files and command
+        mock_fs_response = json.dumps({
+            "files": [{"filename": "test_math.py", "content": "assert True"}],
+            "command": "pytest"
+        })
+        mock_fullstack_llm_instance.generate_response.return_value = mock_fs_response
+
+        # Secure Executor Mock
+        mock_executor_instance = mock_secure_executor.return_value
+        mock_executor_instance.run_command.return_value = {"exit_code": 0, "output": "Passed"}
 
         # Reviewer Mock
         mock_reviewer_llm_instance = mock_reviewer_llm.return_value
@@ -66,36 +69,32 @@ def test_full_chain_flow(self, mock_agent_executor, mock_create_react_agent, moc
         # --- 2. AGENT INITIALIZATION ---
         tech_lead = TechLeadAgent(workspace_path="workspace")
         fullstack = FullstackAgent(workspace_path="workspace")
-        reviewer = CodeReviewAgent()
+        # Reviewer needs to mock _load_prompt_template to avoid file reading error in test env
+        with patch("src.agents.reviewer.agent.CodeReviewAgent._load_prompt_template", return_value="{task_description}"):
+            reviewer = CodeReviewAgent()
+            # Re-attach mock llm because we re-instantiated
+            reviewer.llm = mock_reviewer_llm_instance
 
-        # --- 3. EXECUTION FLOW ---
+            # --- 3. EXECUTION FLOW ---
 
-        # A. PLAN (Tech Lead)
-        print("\n--- 1. Tech Lead Planning ---")
-        plan = tech_lead.create_development_plan("Create a TDD plan for math lib", "python")
+            # A. PLAN (Tech Lead)
+            print("\n--- 1. Tech Lead Planning ---")
+            plan = tech_lead.create_development_plan("Create a TDD plan for math lib", "python")
 
-        assert len(plan.steps) == 2
-        assert plan.steps[0].role == AgentRole.FULLSTACK
+            assert len(plan.steps) == 2
+            assert plan.steps[0].role == AgentRole.FULLSTACK
 
-        # B. STEP 1: EXECUTE (Fullstack)
-        print("\n--- 2. Executing Step 1 ---")
-        step1 = plan.steps[0]
-        # We mock file writing via the executor response, so we don't need real files for this test logic check
-        # But Reviewer needs content. We should patch read_file for reviewer.
+            # B. STEP 1: EXECUTE (Fullstack)
+            print("\n--- 2. Executing Step 1 ---")
+            step1 = plan.steps[0]
 
-        step1_result, modified_files = fullstack.execute_step(step1)
+            step1_result, modified_files = fullstack.execute_step(step1)
 
-        assert step1_result.status == TaskStatus.COMPLETED
-        assert "test_math.py" in modified_files
+            assert step1_result.status == TaskStatus.COMPLETED
+            assert "test_math.py" in modified_files
 
-        # C. STEP 1: REVIEW (Reviewer)
-        print("\n--- 3. Reviewing Step 1 ---")
-        # We need to mock file reading inside reviewer
-        with patch("src.agents.reviewer.agent.CodeReviewAgent._load_prompt_template", return_value="{task_description}"):
-            # Re-init because we patched _load_prompt_template on class
-            reviewer = CodeReviewAgent()
-            # Mock LLM on this new instance
-            reviewer.llm.generate_response.return_value = mock_verdict.model_dump_json()
+            # C. STEP 1: REVIEW (Reviewer)
+            print("\n--- 3. Reviewing Step 1 ---")
 
             verdict = reviewer.review_code(
                 task_description=step1_result.description,
@@ -103,4 +102,4 @@ def test_full_chain_flow(self, mock_agent_executor, mock_create_react_agent, moc
                 execution_logs=step1_result.logs
             )
 
-        assert verdict.verdict == Verdict.PASS
+            assert verdict.verdict == Verdict.PASS
diff --git a/tests/unit/test_architect.py b/tests/unit/test_architect.py
new file mode 100644
index 0000000..f351452
--- /dev/null
+++ b/tests/unit/test_architect.py
@@ -0,0 +1,39 @@
+import pytest
+from unittest.mock import MagicMock, patch
+from src.tools.architect.document_generator import DocumentGeneratorTool
+from src.tools.architect.project_builder import StructureBuilderTool
+
+@pytest.fixture
+def mock_llm():
+    return MagicMock()
+
+@pytest.fixture
+def mock_file_io():
+    return MagicMock()
+
+def test_document_generator(mock_llm):
+    tool = DocumentGeneratorTool(mock_llm)
+    mock_llm.generate_response.return_value = "Markdown content"
+
+    guideline = tool.generate_guideline("Proj", "Desc", "Stack")
+    assert guideline == "Markdown content"
+
+    mock_llm.generate_response.return_value = "`gitignore\nnode_modules\n`"
+    gitignore = tool.generate_gitignore("Node")
+    assert gitignore == "node_modules"
+
+def test_structure_builder_generate(mock_llm, mock_file_io):
+    tool = StructureBuilderTool(mock_llm, mock_file_io)
+    mock_llm.generate_response.return_value = '{"directories": ["src"], "files": ["main.py"]}'
+
+    struct = tool.generate_structure("Context")
+    assert struct["directories"] == ["src"]
+
+def test_structure_builder_build(mock_llm, mock_file_io):
+    tool = StructureBuilderTool(mock_llm, mock_file_io)
+    mock_llm.generate_response.return_value = "print('code')"
+
+    struct = {"directories": ["src"], "files": ["src/main.py"]}
+    tool.build_project(struct, "ctx", "name")
+
+    mock_file_io.write_file.assert_called()
diff --git a/tests/unit/test_core_tools.py b/tests/unit/test_core_tools.py
new file mode 100644
index 0000000..b925dda
--- /dev/null
+++ b/tests/unit/test_core_tools.py
@@ -0,0 +1,73 @@
+import pytest
+from unittest.mock import patch, mock_open, MagicMock
+import os
+from src.tools.core_tools import (
+    write_file, read_file, list_files, execute_command,
+    search_codebase, update_codebase_memory, _resolve_path
+)
+
+@pytest.fixture
+def mock_workspace(tmp_path):
+    # We patch WORKSPACE_PATH where it is defined
+    with patch("src.tools.core_tools.WORKSPACE_PATH", str(tmp_path)):
+        yield tmp_path
+
+def test_resolve_path_valid(mock_workspace):
+    path = _resolve_path("test.txt")
+    assert path == os.path.join(mock_workspace, "test.txt")
+
+def test_resolve_path_traversal(mock_workspace):
+    with pytest.raises(ValueError, match="fora do workspace"):
+        _resolve_path("../../etc/passwd")
+
+def test_write_file_success(mock_workspace):
+    # invoke with string input (filename, content are args) but tool invoke takes dict or str
+    # For multi-arg tools, invoke takes dict
+    result = write_file.invoke({"filename": "test.txt", "content": "hello"})
+    assert "escrito com sucesso" in result
+    assert (mock_workspace / "test.txt").read_text(encoding="utf-8") == "hello"
+
+def test_read_file_success(mock_workspace):
+    (mock_workspace / "test.txt").write_text("hello", encoding="utf-8")
+    content = read_file.invoke("test.txt")
+    assert content == "hello"
+
+def test_read_file_not_found(mock_workspace):
+    result = read_file.invoke("missing.txt")
+    assert "n√£o encontrado" in result
+
+def test_list_files_success(mock_workspace):
+    (mock_workspace / "a.txt").touch()
+    (mock_workspace / "b.txt").touch()
+    result = list_files.invoke(".")
+    assert "a.txt" in result
+    assert "b.txt" in result
+
+@patch("src.tools.core_tools.SecureExecutorTool")
+def test_execute_command_success(MockExecutor, mock_workspace):
+    mock_instance = MockExecutor.return_value
+    mock_instance.run_command.return_value = {"exit_code": 0, "output": "Done"}
+
+    result = execute_command.invoke("echo hi")
+    assert "C√≥digo de Sa√≠da: 0" in result
+    assert "Done" in result
+
+@patch("src.tools.core_tools.VectorMemory")
+def test_search_codebase_success(MockMemory):
+    mock_instance = MockMemory.return_value
+    mock_instance.search.return_value = [("def foo(): pass", {"source": "foo.py"})]
+
+    result = search_codebase.invoke("foo")
+    assert "foo.py" in result
+    assert "def foo(): pass" in result
+
+@patch("src.tools.core_tools.CodeIndexer")
+def test_update_codebase_memory_success(MockIndexer):
+    mock_instance = MockIndexer.return_value
+
+    # invoke() calls the function.
+    result = update_codebase_memory.invoke({})
+    assert "sucesso" in result
+
+    # Check call count
+    assert mock_instance.index_workspace.call_count >= 1
diff --git a/tests/unit/test_file_io.py b/tests/unit/test_file_io.py
new file mode 100644
index 0000000..1523573
--- /dev/null
+++ b/tests/unit/test_file_io.py
@@ -0,0 +1,37 @@
+import pytest
+import os
+from src.tools.file_io import FileIOTool
+
+@pytest.fixture
+def file_io(tmp_path):
+    return FileIOTool(root_path=str(tmp_path))
+
+def test_write_read_file(file_io):
+    file_io.write_file("test.txt", "hello")
+    content = file_io.read_file("test.txt")
+    assert content == "hello"
+
+def test_sanitize_content(file_io):
+    raw = "```python\nprint('hi')\n```"
+    sanitized = file_io._sanitize_content(raw)
+    assert sanitized == "print('hi')"
+
+def test_read_file_sanitization(file_io):
+    # If file on disk has markdown
+    path = os.path.join(file_io.root_path, "md.txt")
+    with open(path, "w") as f:
+        f.write("```python\ncode\n```")
+
+    content = file_io.read_file("md.txt")
+    assert content == "code"
+
+def test_get_project_structure(file_io):
+    file_io.write_file("src/main.py", "print('main')")
+    structure = file_io.get_project_structure()
+    assert "src/" in structure
+    assert "main.py" in structure
+    assert "CONTENT (main.py)" in structure
+
+def test_security_traversal(file_io):
+    with pytest.raises(ValueError):
+        file_io.write_file("../out.txt", "bad")
diff --git a/tests/unit/test_fullstack_agent.py b/tests/unit/test_fullstack_agent.py
index 25870da..53e9205 100644
--- a/tests/unit/test_fullstack_agent.py
+++ b/tests/unit/test_fullstack_agent.py
@@ -1,61 +1,95 @@
 import pytest
-from unittest.mock import MagicMock, patch
+from unittest.mock import MagicMock, patch, mock_open
+import json
 from src.agents.fullstack.agent import FullstackAgent
 from src.core.models import DevelopmentStep, TaskStatus
-from langchain_core.messages import AIMessage
 
 @pytest.fixture
 def mock_fullstack_agent():
-    # Mocking dependencies
     with patch('src.agents.fullstack.agent.LLMProvider'), \
-         patch('src.agents.fullstack.agent.create_react_agent') as MockCreateReactAgent, \
-         patch('src.agents.fullstack.agent.FullstackAgent._load_prompt_template', return_value="Prompt"):
-
-        # Setup graph mock
-        mock_graph = MagicMock()
-        MockCreateReactAgent.return_value = mock_graph
-
+         patch('src.agents.fullstack.agent.FileIOTool'), \
+         patch('src.agents.fullstack.agent.SecureExecutorTool'), \
+         patch('src.agents.fullstack.agent.VectorMemory'), \
+         patch('src.agents.fullstack.agent.CodeIndexer'):
         agent = FullstackAgent(workspace_path="/tmp/test_workspace")
-        agent.graph_mock = mock_graph # Store for assertion
+        agent.llm = MagicMock()
+        agent.file_io = MagicMock()
+        agent.executor = MagicMock()
         return agent
 
-def test_execute_step_success(mock_fullstack_agent):
-    step = DevelopmentStep(id="1", description="Task", role="FULLSTACK")
-
-    # Mock graph response with final message
-    mock_fullstack_agent.agent_executor.invoke.return_value = {
-        "messages": [AIMessage(content="Final Answer")]
+def test_parse_and_save_files_valid(mock_fullstack_agent):
+    """Test parsing a valid response dict with multiple files."""
+    data = {
+        "files": [
+            {"filename": "src/main.py", "content": "print('hello')"},
+            {"filename": "README.md", "content": "# Project"}
+        ],
+        "command": "pytest"
     }
 
-    result_step, modified_files = mock_fullstack_agent.execute_step(step, "Input")
+    created = mock_fullstack_agent._parse_and_save_files(data)
 
-    assert result_step.status == TaskStatus.COMPLETED
-    assert result_step.logs == "Final Answer"
-    assert modified_files == []
+    assert len(created) == 2
+    assert "src/main.py" in created
+    assert "README.md" in created
 
-def test_execute_step_with_files(mock_fullstack_agent):
-    step = DevelopmentStep(id="1", description="Task", role="FULLSTACK")
+    # Verify write_file calls
+    mock_fullstack_agent.file_io.write_file.assert_any_call("src/main.py", "print('hello')")
+    mock_fullstack_agent.file_io.write_file.assert_any_call("README.md", "# Project")
 
-    # Mock graph response with tool call
-    tool_msg = AIMessage(content="I wrote the file.")
-    tool_msg.tool_calls = [{'name': 'write_file', 'args': {'filename': 'test.py'}}]
+def test_parse_and_save_files_invalid_structure(mock_fullstack_agent):
+    """Test handling invalid file list structure."""
+    data = {"files": "invalid_string"}
+    created = mock_fullstack_agent._parse_and_save_files(data)
+    assert created == []
+    mock_fullstack_agent.file_io.write_file.assert_not_called()
 
-    mock_fullstack_agent.agent_executor.invoke.return_value = {
-        "messages": [tool_msg]
+def test_execute_step_success(mock_fullstack_agent):
+    """Test execute_step successful run returning tuple."""
+    step = DevelopmentStep(id="1", description="Implement feature", role="FULLSTACK")
+
+    # Mock LLM Response
+    mock_response = json.dumps({
+        "files": [{"filename": "main.py", "content": "code"}],
+        "command": "python main.py"
+    })
+    mock_fullstack_agent.llm.generate_response.return_value = mock_response
+
+    # Mock Executor Success
+    mock_fullstack_agent.executor.run_command.return_value = {
+        "exit_code": 0,
+        "output": "Success"
     }
 
-    result_step, modified_files = mock_fullstack_agent.execute_step(step, "Input")
+    result_step, files = mock_fullstack_agent.execute_step(step)
 
     assert result_step.status == TaskStatus.COMPLETED
-    assert "test.py" in modified_files
+    assert "Success" in result_step.logs
+    assert files == ["main.py"]
+    mock_fullstack_agent.file_io.write_file.assert_called_with("main.py", "code")
+
+def test_execute_step_retry_logic(mock_fullstack_agent):
+    """Test self-healing retry logic."""
+    step = DevelopmentStep(id="1", description="Implement feature", role="FULLSTACK")
 
-def test_execute_step_failure(mock_fullstack_agent):
-    step = DevelopmentStep(id="1", description="Task", role="FULLSTACK")
+    # Attempt 1: Fail
+    response_fail = json.dumps({"files": [], "command": "fail_cmd"})
+    # Attempt 2: Success
+    response_success = json.dumps({
+        "files": [{"filename": "fixed.py", "content": "fixed"}],
+        "command": "success_cmd"
+    })
 
-    # Mock graph crash
-    mock_fullstack_agent.agent_executor.invoke.side_effect = Exception("Graph Error")
+    mock_fullstack_agent.llm.generate_response.side_effect = [response_fail, response_success]
 
-    result_step, modified_files = mock_fullstack_agent.execute_step(step, "Input")
+    mock_fullstack_agent.executor.run_command.side_effect = [
+        {"exit_code": 1, "output": "Error"},
+        {"exit_code": 0, "output": "Passed"}
+    ]
 
-    assert result_step.status == TaskStatus.FAILED
-    assert "Falha cr√≠tica" in result_step.result
+    result_step, files = mock_fullstack_agent.execute_step(step)
+
+    assert result_step.status == TaskStatus.COMPLETED
+    assert "ATTEMPT 1 FAILED" in result_step.logs  # Should see attempt 1 in logs
+    assert "Passed" in result_step.logs
+    assert "fixed.py" in files
diff --git a/tests/unit/test_graph.py b/tests/unit/test_graph.py
index 54f83db..c67a728 100644
--- a/tests/unit/test_graph.py
+++ b/tests/unit/test_graph.py
@@ -1,7 +1,7 @@
 import pytest
 from unittest.mock import MagicMock, patch
 from src.core.graph.workflow import create_dev_graph, AgentState
-from src.core.models import DevelopmentPlan, DevelopmentStep, TaskStatus, AgentRole, Verdict
+from src.core.models import DevelopmentPlan, DevelopmentStep, TaskStatus, AgentRole, Verdict, CodeReviewVerdict
 
 @pytest.fixture
 def mock_graph_agents():
@@ -11,7 +11,7 @@ def mock_graph_agents():
 
         # Setup TechLead
         tl_instance = MockTL.return_value
-        tl_instance.plan_task.return_value = DevelopmentPlan(
+        tl_instance.create_development_plan.return_value = DevelopmentPlan(
             original_request="Test",
             steps=[
                 DevelopmentStep(id="1", description="Step 1", role=AgentRole.FULLSTACK),
@@ -29,10 +29,9 @@ def execute_side_effect(step, task_input):
 
         # Setup Reviewer
         cr_instance = MockCR.return_value
-        def review_side_effect(step):
-            step.logs = "VERDICT: PASS"
-            return step
-        cr_instance.review_step.side_effect = review_side_effect
+        def review_side_effect(task_description, code_context, execution_logs):
+            return CodeReviewVerdict(verdict=Verdict.PASS, justification="Pass")
+        cr_instance.review_code.side_effect = review_side_effect
 
         yield MockTL, MockFS, MockCR
 
@@ -56,23 +55,16 @@ def test_graph_retry_logic(mock_graph_agents):
     # Reviewer side effect to simulate failure then pass
     cr_instance = MockCR.return_value
 
-    # We use a mutable object to track calls because side_effect iterator is safer
-    # But here we need logic based on which step is being reviewed or retry count.
-    # The reviewer receives `step`.
-
-    # Let's verify that retry logic works if VERDICT is FAIL.
-
     call_count = 0
-    def review_logic(step):
+    def review_logic(task_description, code_context, execution_logs):
         nonlocal call_count
         call_count += 1
         if call_count == 1:
-            step.logs = "VERDICT: FAIL"
+            return CodeReviewVerdict(verdict=Verdict.FAIL, justification="Fail")
         else:
-            step.logs = "VERDICT: PASS"
-        return step
+            return CodeReviewVerdict(verdict=Verdict.PASS, justification="Pass")
 
-    cr_instance.review_step.side_effect = review_logic
+    cr_instance.review_code.side_effect = review_logic
 
     app = create_dev_graph()
     inputs = {"project_path": "./test_workspace", "plan": DevelopmentPlan(original_request="Test")}
@@ -80,5 +72,6 @@ def review_logic(step):
     final_state = app.invoke(inputs)
 
     # Should have retried step 1
-    assert call_count >= 3 # Fail S1, Pass S1, Pass S2
+    # Call count: 1 (Fail S1), 2 (Pass S1), 3 (Pass S2)
+    assert call_count >= 3
     assert final_state["plan"].steps[0].status == TaskStatus.COMPLETED
diff --git a/tests/unit/test_indexer.py b/tests/unit/test_indexer.py
new file mode 100644
index 0000000..b98e50b
--- /dev/null
+++ b/tests/unit/test_indexer.py
@@ -0,0 +1,26 @@
+import pytest
+import os
+from unittest.mock import patch, MagicMock
+from src.core.memory.indexer import CodeIndexer
+
+@patch.dict(os.environ, {"POSTGRES_URL": "postgresql://user:pass@localhost:5432/db"})
+@patch("src.core.memory.indexer.PGVector")
+@patch("src.core.memory.indexer.RecursiveCharacterTextSplitter")
+@patch("src.core.memory.indexer.DirectoryLoader")
+@patch("src.core.memory.indexer.EmbeddingProvider")
+def test_index_workspace(MockEmbed, MockLoader, MockSplitter, MockPGVector, tmp_path):
+    # Setup
+    mock_loader_instance = MockLoader.return_value
+    mock_loader_instance.load.return_value = ["doc"]
+
+    mock_splitter = MockSplitter.return_value
+    mock_splitter.split_documents.return_value = ["split"]
+
+    indexer = CodeIndexer(workspace_path=str(tmp_path))
+    indexer.index_workspace()
+
+    MockPGVector.from_documents.assert_called_once()
+
+def test_index_workspace_invalid_path():
+    with pytest.raises(ValueError):
+        CodeIndexer(workspace_path="/non/existent/path")
diff --git a/tests/unit/test_reviewer_agent.py b/tests/unit/test_reviewer_agent.py
index cb7c1dd..1acb29a 100644
--- a/tests/unit/test_reviewer_agent.py
+++ b/tests/unit/test_reviewer_agent.py
@@ -1,66 +1,56 @@
 import pytest
 from unittest.mock import MagicMock, patch
 from src.agents.reviewer.agent import CodeReviewAgent
-from src.core.models import DevelopmentStep, TaskStatus
+from src.core.models import DevelopmentStep, TaskStatus, Verdict, CodeReviewVerdict
 
 @pytest.fixture
 def mock_reviewer_agent():
     with patch('src.agents.reviewer.agent.LLMProvider'), \
-         patch('src.agents.reviewer.agent.FileIOTool'):
-        agent = CodeReviewAgent(workspace_path="/tmp/test_workspace")
+         patch('src.agents.reviewer.agent.CodeReviewAgent._load_prompt_template', return_value="Prompt: {task_description}"):
+
+        agent = CodeReviewAgent()
         agent.llm = MagicMock()
-        agent.file_io = MagicMock()
         return agent
 
-def test_review_step_pass_verdict(mock_reviewer_agent):
-    """Test that a PASS verdict keeps the step status."""
-    step = DevelopmentStep(id="1", description="Implement feature", role="FULLSTACK", status="COMPLETED")
-    step.logs = "Arquivos gerados: ['main.py']"
-
-    mock_reviewer_agent.file_io.read_file.return_value = "print('hello')"
-    mock_reviewer_agent.llm.generate_response.return_value = "VERDICT: PASS\nGood job."
-
-    reviewed_step = mock_reviewer_agent.review_step(step)
-
-    assert "VERDICT: PASS" in reviewed_step.logs
-    assert reviewed_step.status == TaskStatus.COMPLETED
-
-def test_review_step_fail_verdict(mock_reviewer_agent):
-    """Test that a FAIL verdict is logged (status change is optional in current logic)."""
-    step = DevelopmentStep(id="1", description="Implement feature", role="FULLSTACK", status="COMPLETED")
-    step.logs = "Arquivos gerados: ['main.py']"
-
-    mock_reviewer_agent.file_io.read_file.return_value = "syntax error"
-    mock_reviewer_agent.llm.generate_response.return_value = "VERDICT: FAIL\nSyntax error."
-
-    reviewed_step = mock_reviewer_agent.review_step(step)
-
-    assert "VERDICT: FAIL" in reviewed_step.logs
-    # In the current implementation, it returns the step without changing status to FAILED automatically
-    # unless implemented. This test confirms the current behavior.
-    assert reviewed_step.status == TaskStatus.COMPLETED
-
-def test_review_step_no_files_found(mock_reviewer_agent):
-    """Test fallback when no files are in logs."""
-    step = DevelopmentStep(id="1", description="Task", role="FULLSTACK")
-    step.logs = "No files listed"
-
-    # Mock os.walk to return nothing
-    with patch("os.walk", return_value=[]):
-        reviewed_step = mock_reviewer_agent.review_step(step)
-
-    assert "Nada para revisar" in reviewed_step.logs
-    assert "VERDICT: PASS" in reviewed_step.logs
-
-def test_review_step_llm_crash(mock_reviewer_agent):
-    """Test resilience against LLM failure (Soft Fail)."""
-    step = DevelopmentStep(id="1", description="Task", role="FULLSTACK")
-    step.logs = "Arquivos gerados: ['main.py']"
-
-    mock_reviewer_agent.file_io.read_file.return_value = "code"
+def test_review_code_pass_verdict(mock_reviewer_agent):
+    """Test that a PASS verdict is returned correctly."""
+    # Mock LLM to return valid JSON for CodeReviewVerdict
+    mock_verdict = CodeReviewVerdict(verdict=Verdict.PASS, justification="Looks good")
+    mock_reviewer_agent.llm.generate_response.return_value = mock_verdict.model_dump_json()
+
+    verdict = mock_reviewer_agent.review_code(
+        task_description="Implement feature",
+        code_context="print('hello')",
+        execution_logs="Success"
+    )
+
+    assert verdict.verdict == Verdict.PASS
+    assert verdict.justification == "Looks good"
+
+def test_review_code_fail_verdict(mock_reviewer_agent):
+    """Test that a FAIL verdict is returned correctly."""
+    mock_verdict = CodeReviewVerdict(verdict=Verdict.FAIL, justification="Syntax Error")
+    mock_reviewer_agent.llm.generate_response.return_value = mock_verdict.model_dump_json()
+
+    verdict = mock_reviewer_agent.review_code(
+        task_description="Implement feature",
+        code_context="print(",
+        execution_logs="SyntaxError"
+    )
+
+    assert verdict.verdict == Verdict.FAIL
+    assert verdict.justification == "Syntax Error"
+
+def test_review_code_llm_crash(mock_reviewer_agent):
+    """Test resilience against LLM failure (Fallback to FAIL)."""
     mock_reviewer_agent.llm.generate_response.side_effect = Exception("API Error")
 
-    reviewed_step = mock_reviewer_agent.review_step(step)
+    verdict = mock_reviewer_agent.review_code(
+        task_description="Task",
+        code_context="code",
+        execution_logs="logs"
+    )
 
-    assert "Falha na IA" in reviewed_step.logs
-    assert "VERDICT: PASS (Soft Fail)" in reviewed_step.logs
+    # Agent catches exception and returns FAIL verdict
+    assert verdict.verdict == Verdict.FAIL
+    assert "Falha cr√≠tica" in verdict.justification
diff --git a/tests/unit/test_secure_executor.py b/tests/unit/test_secure_executor.py
new file mode 100644
index 0000000..64f64c8
--- /dev/null
+++ b/tests/unit/test_secure_executor.py
@@ -0,0 +1,36 @@
+import pytest
+from unittest.mock import patch, MagicMock
+from src.tools.secure_executor import SecureExecutorTool
+
+@patch("src.tools.secure_executor.docker.from_env")
+def test_run_command_success(mock_docker):
+    mock_client = MagicMock()
+    mock_docker.return_value = mock_client
+    mock_container = MagicMock()
+    mock_client.containers.run.return_value = mock_container
+
+    mock_container.wait.return_value = {"StatusCode": 0}
+    mock_container.logs.return_value = b"Output"
+
+    executor = SecureExecutorTool(workspace_path="/tmp")
+    result = executor.run_command("echo hi")
+
+    assert result["success"] is True
+    assert result["output"] == "Output"
+    mock_client.containers.run.assert_called_once()
+
+@patch("src.tools.secure_executor.docker.from_env")
+def test_run_command_failure(mock_docker):
+    mock_client = MagicMock()
+    mock_docker.return_value = mock_client
+    mock_container = MagicMock()
+    mock_client.containers.run.return_value = mock_container
+
+    mock_container.wait.return_value = {"StatusCode": 1}
+    mock_container.logs.return_value = b"Error"
+
+    executor = SecureExecutorTool(workspace_path="/tmp")
+    result = executor.run_command("bad cmd")
+
+    assert result["success"] is False
+    assert result["exit_code"] == 1
diff --git a/tests/unit/test_vector_store.py b/tests/unit/test_vector_store.py
new file mode 100644
index 0000000..e5eab2e
--- /dev/null
+++ b/tests/unit/test_vector_store.py
@@ -0,0 +1,27 @@
+import pytest
+import os
+from unittest.mock import patch, MagicMock
+from src.core.memory.vector_store import VectorMemory
+
+@patch.dict(os.environ, {"POSTGRES_URL": "postgresql://user:pass@localhost:5432/db"})
+@patch("src.core.memory.vector_store.PGVector")
+@patch("src.core.memory.vector_store.EmbeddingProvider")
+def test_search(MockEmbed, MockPGVector):
+    mock_store = MockPGVector.return_value
+    # search returns [(doc, score)]
+    mock_doc = MagicMock()
+    mock_doc.page_content = "content"
+    mock_doc.metadata = {"meta": "data"}
+    mock_store.similarity_search_with_score.return_value = [(mock_doc, 0.9)]
+
+    mem = VectorMemory()
+    results = mem.search("query")
+
+    assert len(results) == 1
+    assert results[0][0] == "content"
+    assert results[0][1] == {"meta": "data"}
+
+def test_init_missing_env():
+    with patch.dict(os.environ, {}, clear=True):
+        with pytest.raises(ValueError, match="POSTGRES_URL"):
+            VectorMemory()
